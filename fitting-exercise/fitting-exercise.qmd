---
title: "Fitting Exercise (Week 8)"
---

## Data processing and exploration

Load packages and data
```{r}
#Load packages 
library(here)
library(tidyverse)
library(gtsummary)
library(knitr)
library(tidymodels) 
library(yardstick)
library(pROC)

#read csv data
drug <- read.csv(here("fitting-exercise", "Mavoglurant_A2121_nmpk.csv"))

#preview dataset
dplyr::glimpse(drug) 
```


Make a plot that shows a line for each individual, with DV on the y-axis and time on the x-axis. 
Stratify by dose.
```{r}
#spaghetti plot 
ggplot(drug, aes(x = TIME, y = DV, group = ID, color = factor(DOSE))) +
  geom_line() +
  geom_point() +
  labs(title = "Spaghetti Plot of DV over TIME Stratified by DOSE",
       x = "TIME",
       y = "DV",
       color = "DOSE")
```

Very hard to tell the differences in the trajectory of DV across the DOSE groups,
try log-transforming DV and remake the plot.
```{r}
ggplot(drug, aes(x = TIME, y = log(DV), group = ID, color = factor(DOSE))) +
  geom_line() +
  geom_point() +
  labs(title = "Spaghetti Plot of log(DV) over TIME Stratified by DOSE",
       x = "TIME",
       y = "DV",
       color = "DOSE")
```
It looks like DV decreases faster over time among those from DOSE 25. 


Keeps only observations with OCC = 1.
```{r}
drug2<-drug[drug$OCC==1,]
```


Create a data frame of size 120 x 18 which excludes the observations with TIME = 0 
and contains variable Y (total amount of drug for each individual).
```{r}
#create a new data frame that contains only observations with TIME = 0 
drug3<-drug2[drug2$TIME == 0,]

#exlude observations with TIME = 0 then compute the sum of the DV variable for each individual using dplyr::summarize()
sum_dv <- drug2 %>%
  filter(TIME != 0) %>%
  group_by(ID) %>%
  summarize(Y = sum(DV))

#merge the two data frames above
merged_drug <- left_join(drug3, sum_dv, by = "ID")
```


Converts RACE, SEX and DOSE to factor variables and keeps variables Y,DOSE,AGE,SEX,RACE,WT,HT.
```{r}
merged_drug2<-merged_drug %>%
  mutate(RACE=factor(RACE),
         SEX=factor(SEX),
         DOSE=factor(DOSE)) %>%
  select(Y,DOSE,AGE,SEX,RACE,WT,HT)

#preview new data frame
dplyr::glimpse(merged_drug2) 
```


## EDA revisited

Make a descriptive table of the variables
```{r}
#get descriptive statistics of variables and create a table
table1<-gtsummary::tbl_summary(merged_drug2, statistic = list(
  all_continuous() ~ "{mean}/{sd}",
  all_categorical() ~ "{n} / {N} ({p}%)"
),)
knitr::kable(table1,caption = "Summary Table: Mean/SD or n/N(%)")
```
The majority of the cohort are of DOSE 25, SEX 1,and Race 1.


Make scatterplots between Y and continuous predictors (AGE, WT, HT)
```{r}
#Scatterplot: Y ~ AGE  
ggplot(merged_drug2, aes(AGE, Y)) + 
  geom_point()

#Scatterplot: Y ~ WT  
ggplot(merged_drug2, aes(WT, Y)) + 
  geom_point()

#Scatterplot: Y ~ HT  
ggplot(merged_drug2, aes(HT, Y)) + 
  geom_point()
```
No obvious linear associations observed between Y and AGE, WT, HT.


Make boxplots between Y and categorical predictors (DOSE, SEX, RACE)
```{r}
#Boxplot: Y ~ DOSE  
ggplot(merged_drug2, aes(factor(DOSE), Y)) + 
  geom_boxplot()

#Boxplot: Y ~ SEX  
ggplot(merged_drug2, aes(SEX, Y)) + 
  geom_boxplot()

#Boxplot: Y ~ RACE  
ggplot(merged_drug2, aes(RACE, Y)) + 
  geom_boxplot()
```
Individuals of different RACE groups or SEX groups have similar average value of Y.
There seems to be a positive association between Y and DOSE.


Make histograms for variables Y,AGE, WT, HT.
```{r}
#histogram for Y  
hist(merged_drug2$Y)

#histogram for AGE  
hist(merged_drug2$AGE)

#histogram for WT  
hist(merged_drug2$WT)

#histogram for HT.  
hist(merged_drug2$HT)
```
These variables have a generally normal distribution. No obvious outliers are observed. 


Make scatterplot matrix for variables Y,AGE, WT, HT to inspect correlations.
```{r}
pairs(merged_drug2[, c("Y", "AGE", "WT", "HT")], main = "Scatter Plot Matrix for Y,AGE, WT, HT")
```
WT and HT have relatively high correlation. 


Save the clean data frame locally
```{r}
saveRDS(merged_drug2, file =  here("fitting-exercise", "merged_drug2.rds"))
```

## Model fitting

Fit a linear model to the continuous outcome (Y) using the main predictor of interest DOSE,
and calculate RMSE and R-squared.
```{r}
#fit linear model: Y~DOSE
model1 <- lm(Y ~ DOSE, data = merged_drug2)
summary(model1)

# make predictions on the original data
predictions1 <- predict(model1, newdata = merged_drug2)

# Compute RMSE and R-squared
rmse_1 <- rmse_vec(merged_drug2$Y, predictions1)
rsquared_1 <- rsq_vec(merged_drug2$Y, predictions1)

# print RMSE and R-squared
cat("RMSE:", rmse_1, "\n")
cat("R-squared:", rsquared_1, "\n")
```
Y of DOSE 37.5 (p-value=0.002) and Y of DOSE 50 (p-value<.001) are significantly higher 
than Y of DOSE 25. RMSE is 666.3, and R-squared is 0.516. 


Fit a linear model to the continuous outcome (Y) using the all predictor,and calculate 
RMSE and R-squared.
```{r}
#fit linear model: Y~DOSE+AGE+SEX+RACE+WT+HT
model2 <- lm(Y ~ DOSE+AGE+SEX+RACE+WT+HT, data = merged_drug2)
summary(model2)

# make predictions on the original data
predictions2 <- predict(model2, newdata = merged_drug2)

# Compute RMSE and R-squared
rmse_2 <- rmse_vec(merged_drug2$Y, predictions2)
rsquared_2 <- rsq_vec(merged_drug2$Y, predictions2)

# print RMSE and R-squared
cat("RMSE:", rmse_2, "\n")
cat("R-squared:", rsquared_2, "\n")
```
After adjusting for covariates, Y of DOSE 37.5 (p-value=0.001) and Y of DOSE 50 
(p-value<.001) are significantly higher than Y of DOSE 25. One unit increase in WT (p-value<.001) 
is significantly associated with a decrease of 23.3 in Y. Other covariates are not 
significantly associated with Y. RMSE is 590.3, and R-squared is 0.62. This model 
has better fit than the model with only DOSE as predictor.



Fit a logistic model to the SEX using the main predictor DOSE,and calculate 
accuracy and ROC-AUC.
```{r}
# fit logistic model: SEX~DOSE
logistic_model <- logistic_reg(mode = "classification") %>%
  set_engine("glm") %>%
  set_mode("classification")

model3 <- logistic_model %>% 
  fit(SEX ~ DOSE, data = merged_drug2)

#calculate odds ratios
model_estimates <- tidy(model3, conf.int = TRUE)
model_estimates$odds_ratio <- exp(model_estimates$estimate)

# display model estimates and odds ratios
print(model_estimates[, c("term", "estimate", "conf.low", "conf.high", "odds_ratio","p.value")])


# Make predictions on the original data
prediction3 <- predict(model3, new_data = merged_drug2)
prediction3$SEX<-merged_drug2$SEX

# Compute accuracy
accuracy_value3 <- accuracy(prediction3, truth = SEX, estimate = .pred_class)

# Compute AUC
roc_curve3 <- roc(as.numeric(prediction3$SEX),as.numeric( prediction3$.pred_class))
auc_value3 <- auc(roc_curve3)

# Print accuracy and AUC
cat("Accuracy:", accuracy_value3$.estimate, "\n")
cat("AUC:", auc_value3, "\n")
```
There is no significant association between DOSE and SEX. The accuracy of the model 
is 0.867 and the AUC is 0.5. 



Fit a logistic model to the SEX using the main predictor DOSE,and calculate 
accuracy and ROC-AUC.
```{r}
# fit logistic model: SEX~DOSE
logistic_model <- logistic_reg(mode = "classification") %>%
  set_engine("glm") %>%
  set_mode("classification")

model4 <- logistic_model %>% 
  fit(SEX ~ DOSE + Y + AGE + HT + WT + RACE, data = merged_drug2)

#calculate odds ratios
model_estimates <- tidy(model4, conf.int = TRUE)
model_estimates$odds_ratio <- exp(model_estimates$estimate)

# display model estimates and odds ratios
print(model_estimates[, c("term", "estimate", "conf.low", "conf.high", "odds_ratio","p.value")])


# Make predictions on the original data
prediction4 <- predict(model4, new_data = merged_drug2)
prediction4$SEX<-merged_drug2$SEX

# Compute accuracy
accuracy_value4 <- accuracy(prediction4, truth = SEX, estimate = .pred_class)

# Compute AUC
roc_curve4 <- roc(as.numeric(prediction4$SEX),as.numeric( prediction4$.pred_class))
auc_value4 <- auc(roc_curve4)

# Print accuracy and AUC
cat("Accuracy:", accuracy_value4$.estimate, "\n")
cat("AUC:", auc_value4, "\n")
```
There is still no significant association between DOSE and SEX. Only HT is significantly
associated with SEX. The accuracy of the model is 0.958 and the AUC is 0.897. This model 
has better fit than the model with only DOSE as predictor. 




# Module 10. Model Improvement - Part 1

# Data prep 
Remove variable RACE, and set the seed to 1234.

```{r}
#save a seed value
rngseed = 1234

#read previously saved rds data
module10<-readRDS( here("fitting-exercise", "merged_drug2.rds"))
#remove RACE 
module10 <- subset(module10, select = -RACE)

#set the seed
set.seed(rngseed)

#splits the dataset randomly into a 75% train and 25% test set
data_split <- initial_split(module10, prop = 3/4)
train_data <- training(data_split)
test_data  <- testing(data_split)
```


# Model Fitting
Fit two linear models with Y as outcome, and only DOSE or all variables as predictors, respectively. 

```{r}
#linear models with Y as outcome, DOSE as the only predictor
lin_mod <- linear_reg() %>% set_engine("lm")
linfit1 <- lin_mod %>% fit(Y ~ DOSE, data = train_data)
#linear model with Y as outcome, all variables as predictors
linfit2 <- lin_mod %>% fit(Y ~ ., data = train_data)
```

# Model performance assessment 1

```{r}
#RMSE for model 1
metrics_1 <- linfit1 %>% 
  predict(train_data) %>% 
  bind_cols(train_data) %>% 
  metrics(truth = Y, estimate = .pred) 

print(metrics_1)
```
RMSE when only DOSE as predictor is 702.7909349. 

```{r}
#RMSE for model 2
metrics_2 <- linfit2 %>% 
  predict(train_data) %>% 
  bind_cols(train_data) %>% 
  metrics(truth = Y, estimate = .pred)

print(metrics_2)
```
RMSE when all variables as predictors is 627.2723963.

```{r}
#Manually calculate RMSE for null model
meanY<-mean(train_data$Y)
RSS<-sum((train_data$Y-meanY)^2)
RMSE_null<-sqrt(RSS/nrow(train_data))
print(RMSE_null)
```
RMSE for the null model is 948.3526.

# Model performance assessment 2
Rerun the two models with 10-fold cross-validation. Compute RMSE and standard error of the RMSE.

```{r}
#reset the seed
set.seed(rngseed)

#10 folder Cross-validation
folds <- vfold_cv(train_data, v = 10)

#create work flow 1
rf_wf1 <- 
  workflow() %>%
  add_model(linear_reg()) %>%
  add_formula(Y ~ DOSE)

#Fit model 1 with only DOSE as predictor
set.seed(rngseed)
rf_fit_rs1 <- 
  rf_wf1 %>% 
  fit_resamples(folds)

collect_metrics(rf_fit_rs1)
```
RMSE using 10-fold CV when only DOSE as predictor is 696.7097763, standard error of RMSE is 68.09510508.  


```{r}
#reset the seed
set.seed(rngseed)

#10 folder Cross-validation
folds <- vfold_cv(train_data, v = 10)

#create work flow 2
rf_wf2 <- 
  workflow() %>%
  add_model(linear_reg()) %>%
  add_formula(Y ~ .)

#Fit model 2 with all variables as predictors
set.seed(rngseed)
rf_fit_rs2 <- 
  rf_wf2 %>% 
  fit_resamples(folds)

collect_metrics(rf_fit_rs2)

```
RMSE using 10-fold CV when all variables as predictors is 652.7738843, standard error of RMSE is 63.59876238.  
Comparing RMSE using 10-folder CV to the RMSE early, the RMSE for model 1 decreased (from 702.8 to 696.8), the RMSE for model 2 increased (from 627.3 to 652.8). The RMSE for the model with DOSE as only predictor is still higher than that of the model with all variables as predictors. This indicates that the model with all variables still has better performance than the one with DOSE only; and the model with DOSE only has better performance than the null model. 

# Rerun the cross-validation with a different seed

```{r}
#Rerun model 1 with a different seed
#different seed 
seed2 = 8876

#reset the seed
set.seed(seed2)

#10 folder Cross-validation
folds <- vfold_cv(train_data, v = 10)

#create work flow 1
rf_wf1b <- 
  workflow() %>%
  add_model(linear_reg()) %>%
  add_formula(Y ~ DOSE)

#Fit model 1 with only DOSE as predictor
set.seed(seed2)
rf_fit_rs1b <- 
  rf_wf1b %>% 
  fit_resamples(folds)

collect_metrics(rf_fit_rs1b)

```
With a different seed, RMSE using 10-fold CV when only DOSE as predictor is 714.5896655, standard error of RMSE is 34.92669480.


```{r}
#Rerun model 2 with a different seed
#reset the seed
set.seed(seed2)

#10 folder Cross-validation
folds <- vfold_cv(train_data, v = 10)

#create work flow 2
rf_wf2b <- 
  workflow() %>%
  add_model(linear_reg()) %>%
  add_formula(Y ~ .)

#Fit model 1 with only DOSE as predictor
set.seed(seed2)
rf_fit_rs2b <- 
  rf_wf2b %>% 
  fit_resamples(folds)

collect_metrics(rf_fit_rs2b)
```
With a different seed, RMSE using 10-fold CV when all variables as predictors is 655.797776, standard error of RMSE is 54.67215006.

The overall pattern between changes in the RMSE for the models trained without CV and the models trained with CV is the same. Model with all variables still performs better than the model with DOSE only. 

---

## This section added by Liza Hall 

\

### Model Predictions
#### Predicted Values VS Observed Values
Load necessecary libraries

```{r}
library(tidyverse)
library(tidymodels)
```

Generate predictions for both models.

```{r}
# Prediction for Model 1 (DOSE as predictor)
pred1 <- predict(linfit1, train_data) %>%
  bind_cols(train_data) %>%
  select(Y, .pred) %>%
  mutate(model = "DOSE Only")

# Prediction for Model 2 (All variables as predictors)
pred2 <- predict(linfit2, train_data) %>%
  bind_cols(train_data) %>%
  select(Y, .pred) %>%
  mutate(model = "All Predictors")
```

Manually create predictions for null model based on the mean.

```{r}
# Prediction for Null Model (mean prediction)
train_data$null_pred <- meanY # meanY calculated previously as mean(train_data$Y)
null_model_pred <- train_data %>%
  select(Y, null_pred) %>%
  rename(.pred = null_pred) %>%
  mutate(model = "Null Model")

train_data <- train_data %>% select(-null_pred)
```

Combine all predictions into one dataset. 

```{r}
# Combine all predictions into one data frame
combined_predictions <- bind_rows(pred1, pred2, null_model_pred)
```

Plotting the graph with ggplot.

```{r, warning=FALSE}
# Plotting with ggplot
ggplot(combined_predictions, aes(x = Y, y = .pred, color = model)) +
  geom_point() +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "gray") + # 45 degree line
  scale_x_continuous(limits = c(0, 5000)) +
  scale_y_continuous(limits = c(0, 5000)) +
  labs(x = "Observed", y = "Predicted", title = "Observed vs. Predicted Values") +
  theme_minimal() +
  scale_color_manual(values = c("DOSE Only" = "#5c88da", "All Predictors" = "#84bd00", "Null Model" = "#ffcd00")) +
  theme(
    plot.background = element_rect(fill = "gray99", color = NA), # Change the entire plot background
    panel.background = element_rect(fill = "gray99", color = NA) # Change the plotting area background
  )
```

#### Predicted Values VS Residuals

Calculate residuals for model 2.

```{r}
# Calculate residuals for Model 2
model2_residuals <- pred2 %>%
  mutate(residuals = .pred - Y)
```

Finding the range for the Y-axis.
\
This ensures it goes into the positive and negative by the same amount.

```{r}
# Finding the range for Y-axis to ensure it goes the same amount into positive and negative direction
max_abs_residual <- max(abs(model2_residuals$residuals))
```

Plotting the graph with ggplot.

```{r}
# Plotting Predicted vs. Residuals for Model 2
ggplot(model2_residuals, aes(x = .pred, y = residuals)) +
  geom_point(colour="#5c88da") +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") + # Straight line at 0
  scale_y_continuous(limits = c(-max_abs_residual, max_abs_residual)) + # Same amount in positive and negative direction
  labs(x = "Predicted", y = "Residuals", title = "Predicted vs. Residuals for All Predictors Model") +
  theme_minimal() +
  theme(
    plot.background = element_rect(fill = "gray99", color = NA), # Change the entire plot background
    panel.background = element_rect(fill = "gray99", color = NA) # Change the plotting area background
  )
```

### Model Predictions and Uncertainty

\

#### Predicted Values VS Observed Values

Load required libraries.
```{r}
library(rsample)
```

Set seed and create bootstraps.

```{r}
set.seed(rngseed)

dat_bs <- bootstraps(train_data, times = 100)
```

Initialize list to store predictions, then loop over bootstrap samples and store predcitions in list. Also, assigning original predictions to 'orig_pred' for easier managment. 

```{r, warning=FALSE}
# Initialize a list to store predictions
pred_bs <- list()

# Loop over each bootstrap sample in 'dat_bs'
for(i in 1:length(dat_bs$splits)) {
 
  # Extract the bootstrap sample
  dat_sample = rsample::analysis(dat_bs$splits[[i]])
  
  # Fit the model to the bootstrap sample 
  linfit2 <- lin_mod %>% fit(Y ~ ., data = dat_sample)
  
  # Make predictions on the original training data
  predictions <- predict(linfit2, new_data = train_data)
  
  # Store predictions
  pred_bs[[i]] <- predictions
}

# Assigning original predictions 
orig_pred <- pred2$.pred
```

Convert vector list into matrix. Then the mean, lower CI, and upper CI are calculated for each row of bootstrap samples. After this is done, the values are assigned to 'train_data' for plotting. 

```{r}
# Convert the list of vectors into a matrix
pred_matrix <- do.call(cbind, pred_bs)

# Calculate the median, lower CI, and upper CI for each row across bootstrap samples
preds <- apply(pred_matrix, 1, function(x) quantile(x, probs = c(0.025, 0.5, 0.975)))
preds <- t(preds) 

# Assign
train_data$median_pred <- preds[, 2] # Median
train_data$lower_ci <- preds[, 1] # Lower CI
train_data$upper_ci <- preds[, 3] # Upper CI
```

Plotting the graph. 

```{r}
# Plotting
ggplot(train_data, aes(x = Y)) +
  geom_point(aes(y = orig_pred, color = "Original Predictions"), size = 1.5) +
  geom_point(aes(y = median_pred, color = "Median Predictions"), size = 1.5) +
  geom_errorbar(aes(ymin = lower_ci, ymax = upper_ci, color = "Confidence Intervals"), width = 0.2, alpha = 0.8) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "gray") +
  scale_x_continuous(limits = c(0, max(train_data$Y))) +
  scale_y_continuous(limits = c(0, max(c(train_data$orig_pred, train_data$upper_ci)))) +
  labs(x = "Observed Values", y = "Predicted Values with Confidence Intervals", 
       title = "Observed Values vs. Predicted Values with Confidence Intervals") +
  scale_color_manual(values = c("Original Predictions" = "#84bd00",
                                "Median Predictions" = "#5c88da",
                                "Confidence Intervals" = "#ffcd00")) +
  theme_minimal() +
   theme(
    plot.background = element_rect(fill = "gray99", color = NA), # Change the entire plot background
    panel.background = element_rect(fill = "gray99", color = NA) # Change the plotting area background
  ) +
  guides(color = guide_legend(title = "Legend")) +
  theme(legend.position = "right", 
        legend.text = element_text(size = 8), 
        legend.title = element_text(size = 9), 
        legend.key.size = unit(0.5, "cm"), 
        legend.spacing.x = unit(0.2, "cm"), 
        legend.spacing.y = unit(0.1, "cm")) 
```

\

#### Interpretation 

As seen in the graph above, the points on the graph are relatively close to the line of perfect fit, which suggests that the model's predictions are reasonably accurate. The model does seem to be better at predicting the lower values of Y however, as seen by the cluster of points near the line of perfect fit, towards the bottom left of the graph. As the values of Y get higher, however, the points become more scattered, indicating that the model is not at good at predicting the higher Y values. There also somewhat appears to be a pattern in the error bars, which also indicates that the model may be better at predicting the lower values of Y. 



# Part 3

## Final evaluation using test data

Use model 2 to make predictions for the test data and create a dataframe that contains
observed data and predicted data for both train data and test data. 
```{r}
#make predictions using test data
pred_test <- predict(linfit2, new_data = test_data)$.pred

#create dataframes for predictions on train data and test data respectively
test_pred <- data.frame(
  Observed  = test_data$Y,
  Predicted  = pred_test,
  label="Predictions on test data")

train_pred <- data.frame(
  Observed  = train_data$Y,
  Predicted  = pred2$.pred,
  label="Predictions on train data")

#concatenate two dataframes above
graph_3<-rbind(train_pred,test_pred)
```

Plot predicted versus observed values for both the train data and test data

```{r}
ggplot(graph_3, aes(x = Observed, y = Predicted, color = label, shape=label)) +
  geom_point() +  
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "grey") +  
  labs(x = "Observed Values", y = "Predicted Values", title = "Observed vs. Predicted Values by Train/Test data", color = "Train/Test Data", shape ="Train/Test Data" ) +
  theme_minimal() + # Use a minimal theme for aesthetics
  scale_shape_manual(values = c(15, 17)) + # Set different shapes for train/test predictions
  scale_color_manual(values = c("blue", "green"))
```

## Overall model assessment
Model 1(DOSE) and Model 2(all predictors) both have much lower RMSE than the null model,
regardless of fitting the model directly to train data or using cross-validation. This indicates 
Model 1 and Model 2 have better performance than the null model. 

Although Model 1 performs better than the null model, it does not predicts well. In the observed vs predicted value plot, predictions from Model 1 form three horizontal lines along the diagonal reference line. Model 1 is not able to capture the main pattern in the data. This is probably due 
to the lack of variation in DOSE which has only three values. I would not use Model 1 to make predictions for real purposes.

Model 2 has better performance with additional predictors added. It has the lowest RMSE; the data 
points on observed vs predicted plot generally scatter around the diagonal line, the confidence intervals for predictions are not too wide; and there is no obvious odd pattern in the residual vs predicted value plot. This indicates Model 2 performs well and can captures the main pattern in the data. Most importantly, the data points on the observed vs predicted plot using the test data scatter around the diagonal line, which indicates the predictions on the test data are generally accurate. I would use this model to make predictions. But I would be careful for predicted values that are too high (roughly over 4000),as the observed vs predicted plot indicates lower accuracy for that range.   



